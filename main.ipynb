{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import eng_to_ipa as ipa\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding, RepeatVector\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import load_model\n",
    "from keras import optimizers\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras.models import  Sequential, Model\n",
    "from tensorflow.keras.layers import Layer, Concatenate, Input, Masking, LSTM, Embedding, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data (data file already created in data_import.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeEncodeError",
     "evalue": "'charmap' codec can't encode characters in position 9-10: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m                        Traceback (most recent call last)",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ipa\u001b[38;5;241m.\u001b[39misin_cmu(word):\n\u001b[0;32m     10\u001b[0m         line \u001b[38;5;241m=\u001b[39m word\u001b[38;5;241m.\u001b[39mstrip(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m ipa\u001b[38;5;241m.\u001b[39mconvert(word) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 11\u001b[0m         \u001b[43mfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m file\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mf:\\college stuff\\python\\lib\\encodings\\cp1252.py:19\u001b[0m, in \u001b[0;36mIncrementalEncoder.encode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m---> 19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcodecs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcharmap_encode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43mencoding_table\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m: 'charmap' codec can't encode characters in position 9-10: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "file = open('words.txt','r')\n",
    "lines = file.readlines()\n",
    "file.close()\n",
    "\n",
    "file = open('data.csv','w')\n",
    "\n",
    "file.write('word,pronunciation\\n')\n",
    "for word in lines:\n",
    "    if ipa.isin_cmu(word):\n",
    "        line = word.strip('\\n') + ',' + ipa.convert(word) + '\\n'\n",
    "        file.write(line)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of initial data: 40315\n",
      "Train size: 32253\n",
      "Val size: 4031\n",
      "Test size: 4031\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4031, 23)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv ('data.csv',delimiter=',')\n",
    "\n",
    "df.word = df.word.astype(str) \n",
    "df.pronunciation = df.pronunciation.astype(str) \n",
    "\n",
    "#df.applymap(lambda s: s.replace(s,' '.join(str(s))))\n",
    "df['word'] = df['word'].str.replace('',' ')\n",
    "df['pronunciation'] = df['pronunciation'].str.replace('',' ')\n",
    "\n",
    "MAX_NUM_WORDS = 40000\n",
    "\n",
    "# 10% for val, 10% for test, 70% for train\n",
    "val_size = int(df.shape[0] * 0.1)\n",
    "test_size = int(df.shape[0] * 0.1)\n",
    "\n",
    "# Shuffle the data\n",
    "df = df.sample(frac=1)\n",
    "# Split df to test/val/train\n",
    "test_df = df[:test_size]\n",
    "val_df = df[test_size:test_size+val_size]\n",
    "train_df = df[test_size+val_size:]\n",
    "\n",
    "\n",
    "train_words, train_pronounciations = list(train_df.word), list(train_df.pronunciation)\n",
    "val_words, val_pronounciations     = list(val_df.word), list(val_df.pronunciation)\n",
    "test_words, test_pronounciations   = list(test_df.word), list(test_df.pronunciation)\n",
    "\n",
    "\n",
    "# Check that idces do not overlap\n",
    "assert set(train_df.index).intersection(set(val_df.index)) == set({})\n",
    "assert set(test_df.index).intersection(set(train_df.index)) == set({})\n",
    "assert set(val_df.index).intersection(set(test_df.index)) == set({})\n",
    "# Check that all idces are present\n",
    "assert df.shape[0] == len(train_pronounciations) + len(val_pronounciations) + len(test_pronounciations)\n",
    "\n",
    "# Sizes\n",
    "print(\n",
    "    f\"Size of initial data: {df.shape[0]}\\n\"\n",
    "    f\"Train size: {len(train_pronounciations)}\\n\"\n",
    "    f\"Val size: {len(val_pronounciations)}\\n\"\n",
    "    f\"Test size: {len(test_pronounciations)}\\n\"\n",
    ")\n",
    "\n",
    "for i in range(len(train_pronounciations)):\n",
    "    train_pronounciations[i] = \"<START>\" + train_pronounciations[i] + \"<END>\"\n",
    "\n",
    "for i in range(len(val_pronounciations)):\n",
    "    val_pronounciations[i] = \"<START>\" + val_pronounciations[i] + \"<END>\"\n",
    "\n",
    "for i in range(len(test_pronounciations)):\n",
    "    test_pronounciations[i] = \"<START>\" + test_pronounciations[i] + \"<END>\"\n",
    "\n",
    "ipa_tokenizer = Tokenizer(num_words = MAX_NUM_WORDS, filters = '')\n",
    "ipa_tokenizer.fit_on_texts(train_pronounciations)\n",
    "ipa_int_seq = ipa_tokenizer.texts_to_sequences(train_pronounciations)\n",
    "\n",
    "ipa_word_to_indx = ipa_tokenizer.word_index\n",
    "\n",
    "max_ipa_len = max(len(sen) for sen in ipa_int_seq)\n",
    "\n",
    "padded_tokenized_ipa = tf.keras.preprocessing.sequence.pad_sequences(ipa_int_seq, maxlen = max_ipa_len, padding = 'post', value = 0)\n",
    "\n",
    "padded_tokenized_ipa.shape\n",
    "\n",
    "\n",
    "\n",
    "val_tokenizer = Tokenizer(num_words = MAX_NUM_WORDS, filters = '')\n",
    "val_tokenizer.fit_on_texts(val_pronounciations)\n",
    "val_int_seq = val_tokenizer.texts_to_sequences(val_pronounciations)\n",
    "\n",
    "val_word_to_indx = ipa_tokenizer.word_index\n",
    "\n",
    "max_val_len = max(len(sen) for sen in val_int_seq)\n",
    "\n",
    "padded_tokenized_val = tf.keras.preprocessing.sequence.pad_sequences(val_int_seq, maxlen = max_val_len, padding = 'post', value = 0)\n",
    "\n",
    "padded_tokenized_val.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(TensorSpec(shape=(None, None, 128), dtype=tf.float32, name=None), TensorSpec(shape=(None, 24), dtype=tf.int32, name=None))\n",
      "(TensorSpec(shape=(None, None, 128), dtype=tf.float32, name=None), TensorSpec(shape=(None, 23), dtype=tf.int32, name=None))\n",
      "(16, 13, 128)\n",
      "tf.Tensor(\n",
      "[[ 1 23  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 1  3 18 19 37 13 20 23 12  8  5 15  2  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 1  4  9  3 16 10 17 15  4  9  7 10 14  2  0  0  0  0  0  0  0  0  0]\n",
      " [ 1  3 33 17  7 13  8 31  7  2  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 1 18 29  6  8  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 1  3 18 23 11  4  6 13 21  5  7  4  6 15  2  0  0  0  0  0  0  0  0]\n",
      " [ 1  3  9  4  6 37  4  6  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 1 12  5 10  7  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 1  3  5  9 24  4  9  7  8  2  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 1 11 17 10 27 15  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 1  3 20  4  6  9  4  6 15  2  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 1  4  3  8  7 31  7  9  4  8  2  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 1 12  6 23 16  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 1 20 14 12  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 1  3 16  5  6 14  4 11 15  2  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 1  3  8 25  5 12  4 10 15  2  0  0  0  0  0  0  0  0  0  0  0  0  0]], shape=(16, 23), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_data = tf.data.Dataset.from_tensor_slices((train_words, padded_tokenized_ipa))\n",
    "valid_data = tf.data.Dataset.from_tensor_slices((val_words, padded_tokenized_val))\n",
    "\n",
    "def str_split(e, g):\n",
    "    e = tf.strings.split(e)\n",
    "    return e, g\n",
    " \n",
    "train_data = train_data.map(str_split)\n",
    "valid_data = valid_data.map(str_split)\n",
    "       \n",
    "embedding_layer = hub.load(\"https://tfhub.dev/google/tf2-preview/nnlm-en-dim128-with-normalization/1\")    \n",
    "    \n",
    "def embed_english(x, y):\n",
    "    return embedding_layer(x), y\n",
    " \n",
    "train_data = train_data.map(embed_english)\n",
    "valid_data = valid_data.map(embed_english)\n",
    "\n",
    "\n",
    "def remove_long_sentence(e, g):\n",
    "    return tf.shape(e)[0] <= 13\n",
    " \n",
    "train_data = train_data.filter(remove_long_sentence)\n",
    "valid_data = valid_data.filter(remove_long_sentence)\n",
    "\n",
    "\n",
    "def pad_english(e, g):\n",
    "    return tf.pad(e, paddings = [[13-tf.shape(e)[0],0], [0,0]], mode='CONSTANT', constant_values=0), g\n",
    " \n",
    "train_data = train_data.map(pad_english)\n",
    "valid_data = valid_data.map(pad_english)\n",
    "\n",
    "train_data = train_data.batch(16)\n",
    "valid_data = valid_data.batch(16)\n",
    "\n",
    "print(train_data.element_spec)\n",
    "print(valid_data.element_spec)\n",
    "\n",
    "for e, g in train_data.take(1):\n",
    "    print(e.shape)\n",
    " \n",
    "for e, g in valid_data.take(1):\n",
    "    print(g)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLayer(Layer):\n",
    " \n",
    "    def __init__(self, **kwargs):\n",
    "        super(CustomLayer, self).__init__(**kwargs)\n",
    "        self.embed = tf.Variable(initial_value=tf.zeros(shape=(1,128)), trainable=True, dtype='float32')\n",
    "         \n",
    "    def call(self, inputs):\n",
    "        x = tf.tile(self.embed, [tf.shape(inputs)[0], 1])\n",
    "        x = tf.expand_dims(x, axis=1)\n",
    "        return tf.concat([inputs, x], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 13, 128)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([16, 14, 128])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_layer = CustomLayer()\n",
    "e, g = next(iter(train_data.take(1)))\n",
    "print(e.shape)\n",
    "o = custom_layer(e)\n",
    "o.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(batch_shape = (None, 13, 128), name='input')\n",
    "x = CustomLayer(name='custom_layer')(inputs)\n",
    "x = Masking(mask_value=0, name='masking_layer')(x)\n",
    "x, h, c = LSTM(units=512, return_state=True, name='lstm')(x)\n",
    "encoder_model = Model(inputs = inputs, outputs = [h, c], name='encoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "#tokenizer = Tokenizer(train_words)\n",
    "#pronounciation_tokenizer = Tokenizer(train_pronounciations)\n",
    "#print(word_tokenizer.index_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 24, 41) (16, 512) (16, 512)\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_layer (Embedding)  multiple                 5248      \n",
      "                                                                 \n",
      " lstm_layer (LSTM)           multiple                  1312768   \n",
      "                                                                 \n",
      " dense_layer (Dense)         multiple                  21033     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,339,049\n",
      "Trainable params: 1,339,049\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "class Decoder(Model):\n",
    "     \n",
    "    def __init__(self, **kwargs):\n",
    "        super(Decoder, self).__init__(**kwargs)\n",
    "        self.embed = Embedding(input_dim=len(ipa_tokenizer.index_word)+1, output_dim=128, mask_zero=True, name='embedding_layer')\n",
    "        self.lstm = LSTM(units = 512, return_state = True, return_sequences = True, name='lstm_layer')\n",
    "        self.dense = Dense(len(ipa_tokenizer.index_word)+1, name='dense_layer')\n",
    "         \n",
    "    def call(self, inputs, hidden_state = None, cell_state = None):\n",
    "        x = self.embed(inputs)\n",
    "        x, hidden_state, cell_state = self.lstm(x, initial_state = [hidden_state, cell_state]) \\\n",
    "                                                     if hidden_state is not None and cell_state is not None else self.lstm(x)\n",
    "        x = self.dense(x)\n",
    "        return x, hidden_state, cell_state\n",
    " \n",
    "decoder_model = Decoder(name='decoder')\n",
    "e, g_in = next(iter(train_data.take(1)))\n",
    "h, c = encoder_model(e)\n",
    "g_out, h, c = decoder_model(g_in, h, c)\n",
    " \n",
    "print(g_out.shape, h.shape, c.shape)\n",
    "\n",
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ipa_decoder_data(g):\n",
    "    g1 = g.numpy()\n",
    "    #print(g1)\n",
    "    #new_g1 =  np.delete(g1, np.where(g1 == 2), axis = 0)\n",
    "    '''\n",
    "    for list in g1:\n",
    "        for int in range(len(list)):\n",
    "            if list[int] == 2:\n",
    "                np.delete(list, [int])\n",
    "    '''\n",
    "    \n",
    "    x, y = g1.shape\n",
    "    #after delete\n",
    "    new_g1 = g1[g1 != 2]\n",
    "    new_g1 = np.reshape(new_g1, (x, y-1))\n",
    "\n",
    "    #print(new_g1)\n",
    "    \n",
    "    g2 = g.numpy()\n",
    "    new_g2 =  np.delete(g2,0, axis = 1)\n",
    "    new_g1 = np.pad(new_g1, ((0,0),(1,0)), 'constant')\n",
    "    g_in = tf.convert_to_tensor(new_g1, dtype=tf.int32)\n",
    "    \n",
    "    #print(g_in)\n",
    "    new_g2 = np.pad(new_g2, ((0,0),(0,1)), 'constant')\n",
    "    g_out = tf.convert_to_tensor(new_g2, dtype=tf.int32)\n",
    "    #print(g_out)\n",
    "    return g_in, g_out\n",
    "\n",
    "@tf.function\n",
    "def forward_backward(encoder_model, decoder_model, e, g_in, g_out, loss):\n",
    "    with tf.GradientTape() as tape:\n",
    "        h, c = encoder_model(e)\n",
    "        d_g_out, _, _ = decoder_model(g_in, h, c)\n",
    "        cur_loss = loss(g_out, d_g_out)\n",
    "        grads = tape.gradient(cur_loss, encoder_model.trainable_variables + decoder_model.trainable_variables)\n",
    "    return cur_loss, grads\n",
    " \n",
    "def train_encoder_decoder(encoder_model, decoder_model, num_epochs, train_data, valid_data, valid_steps, \n",
    "                          optimizer, loss, grad_fn):\n",
    "    train_losses = []\n",
    "    val_loasses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        train_epoch_loss_avg = tf.keras.metrics.Mean()\n",
    "        val_epoch_loss_avg = tf.keras.metrics.Mean()\n",
    "        for e, g in train_data:\n",
    "            g_in, g_out = get_ipa_decoder_data(g)\n",
    "            #print(g_out)\n",
    "            train_loss, grads = grad_fn(encoder_model, decoder_model, e, g_in, g_out, loss)\n",
    "            optimizer.apply_gradients(zip(grads, encoder_model.trainable_variables + decoder_model.trainable_variables))\n",
    "            train_epoch_loss_avg.update_state(train_loss)    \n",
    "        for e_v, g_v in valid_data.take(valid_steps):\n",
    "            g_v_in, g_v_out = get_german_decoder_data(g_v)\n",
    "            val_loss, _ = grad_fn(encoder_model, decoder_model, e_v, g_v_in, g_v_out, loss)\n",
    "            val_epoch_loss_avg.update_state(val_loss)        \n",
    "        print(f'epoch: {epoch}, train loss: {train_epoch_loss_avg.result()}, validation loss: {val_epoch_loss_avg.result()}')    \n",
    "        train_losses.append(train_epoch_loss_avg.result())\n",
    "        val_loasses.append(val_epoch_loss_avg.result())\n",
    "    return train_losses, val_loasses\n",
    " \n",
    "optimizer_obj = Adam(learning_rate = 1e-3)\n",
    "loss_obj = SparseCategoricalCrossentropy(from_logits=True)\n",
    "train_loss_results, valid_loss_results = train_encoder_decoder(encoder_model, decoder_model, 20, train_data, valid_data, 20,\n",
    "                                                          optimizer_obj, loss_obj, forward_backward)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.xlabel(\"Epochs\", fontsize=14)\n",
    "plt.ylabel(\"Loss\", fontsize=14)\n",
    "plt.title('Loss vs epochs')\n",
    "plt.plot(train_loss_results, label='train')\n",
    "plt.plot(valid_loss_results, label='valid')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 102 3256 3141  838 3471]\n",
      "TensorSpec(shape=(None, 128), dtype=tf.float32, name=None)\n",
      "tf.Tensor(\n",
      "[[ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " ...\n",
      " [ 0.12092672  0.11381436 -0.06334078 ...  0.01318553  0.0630509\n",
      "  -0.13633421]\n",
      " [ 0.05008225  0.11326285  0.16089416 ...  0.13769226 -0.00445221\n",
      "  -0.1637492 ]\n",
      " [ 0.11089496  0.05322636 -0.07900235 ...  0.06433398 -0.03737464\n",
      "  -0.15818046]], shape=(13, 128), dtype=float32)\n",
      "\n",
      "102\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Exception encountered when calling layer \"lstm_layer\" (type LSTM).\n\nslice index 0 of dimension 0 out of bounds. [Op:StridedSlice] name: decoder/lstm_layer/strided_slice/\n\nCall arguments received:\n  • inputs=tf.Tensor(shape=(1, 0, 128), dtype=float32)\n  • mask=tf.Tensor(shape=(1, 0), dtype=bool)\n  • training=None\n  • initial_state=['tf.Tensor(shape=(1, 512), dtype=float32)', 'tf.Tensor(shape=(1, 512), dtype=float32)']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Input \u001b[1;32mIn [79]\u001b[0m, in \u001b[0;36m<cell line: 28>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     36\u001b[0m g_t \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     37\u001b[0m g_in \u001b[38;5;241m=\u001b[39m start_token\n\u001b[1;32m---> 38\u001b[0m g_out, h, c \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg_in\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m g_t\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     40\u001b[0m g_out \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39margmax(g_out, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[1;32mf:\\college stuff\\python\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Input \u001b[1;32mIn [29]\u001b[0m, in \u001b[0;36mDecoder.call\u001b[1;34m(self, inputs, hidden_state, cell_state)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, hidden_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, cell_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     10\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed(inputs)\n\u001b[1;32m---> 11\u001b[0m     x, hidden_state, cell_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_state\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mhidden_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcell_state\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \\\n\u001b[0;32m     12\u001b[0m                                                  \u001b[38;5;28;01mif\u001b[39;00m hidden_state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m cell_state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm(x)\n\u001b[0;32m     13\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense(x)\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x, hidden_state, cell_state\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Exception encountered when calling layer \"lstm_layer\" (type LSTM).\n\nslice index 0 of dimension 0 out of bounds. [Op:StridedSlice] name: decoder/lstm_layer/strided_slice/\n\nCall arguments received:\n  • inputs=tf.Tensor(shape=(1, 0, 128), dtype=float32)\n  • mask=tf.Tensor(shape=(1, 0), dtype=bool)\n  • training=None\n  • initial_state=['tf.Tensor(shape=(1, 512), dtype=float32)', 'tf.Tensor(shape=(1, 512), dtype=float32)']"
     ]
    }
   ],
   "source": [
    "english = test_words\n",
    "\n",
    "#print(english)\n",
    "\n",
    "indices = np.random.choice(len(english), 5)\n",
    "\n",
    "print(indices)\n",
    "\n",
    "test_data = tf.data.Dataset.from_tensor_slices(np.array([english[i] for i in indices]))\n",
    "test_data = test_data.map(tf.strings.split)\n",
    "test_data = test_data.map(embedding_layer)\n",
    "test_data = test_data.filter(lambda x: tf.shape(x)[0] <= 13)\n",
    "test_data = test_data.map(lambda x: tf.pad(x, paddings = [[13-tf.shape(x)[0],0], [0,0]], mode='CONSTANT', constant_values=0))\n",
    "print(test_data.element_spec)\n",
    "# TensorSpec(shape=(None, 128), dtype=tf.float32, name=None)\n",
    "\n",
    "#for x in test_data:\n",
    "    #print(x)\n",
    "    \n",
    "#testing = test_data.take(6)\n",
    "#for x in testing:\n",
    "    #print(x)\n",
    "    \n",
    "n = 0\n",
    "\n",
    "start_token = np.array(ipa_tokenizer.texts_to_sequences(['']))\n",
    "end_token = np.array(ipa_tokenizer.texts_to_sequences(['']))\n",
    "for e, i in zip(test_data.take(5), indices):\n",
    "    #print(test_data.take(0))\n",
    "    \n",
    "    print(e)\n",
    "    print()\n",
    "    print(i)\n",
    "    \n",
    "    h, c = encoder_model(tf.expand_dims(e, axis=0))\n",
    "    g_t = []\n",
    "    g_in = start_token\n",
    "    g_out, h, c = decoder_model(g_in, h, c)\n",
    "    g_t.append('')\n",
    "    g_out = tf.argmax(g_out, axis=2)\n",
    "    while g_out != end_token: \n",
    "        g_out, h, c = decoder_model(g_in, h, c)\n",
    "        g_out = tf.argmax(g_out, axis=2)\n",
    "        g_in = g_out\n",
    "        g_t.append(ipa_tokenizer.index_word.get(tf.squeeze(g_out).numpy(), 'UNK'))\n",
    "    print(f'English Text: {english[i]}')\n",
    "    print(f'German Translation: {\" \".join(g_t)}')\n",
    "    print()\n",
    "    n += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize\n",
    "#word_tokenizer = Tokenizer(train_words)\n",
    "#pronounciation_tokenizer = Tokenizer(train_pronounciations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = Sequential()\n",
    "#model.add(Embedding(len(train_words), 512, input_length=20, mask_zero=True))\n",
    "#model.add(LSTM(512))\n",
    "#model.add(RepeatVector(20))\n",
    "#model.add(LSTM(512, return_sequences=True))\n",
    "#model.add(Dense(len(train_pronounciations), activation='softmax'))\n",
    " \n",
    "#rms = keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "#model.compile(optimizer=rms, loss='sparse_categorical_crossentropy')\n",
    "\n",
    "#model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6a251302415591a00991e803008012c897957666279c59a7b2e49cbcc4b50ee2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
