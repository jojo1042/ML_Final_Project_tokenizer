{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import eng_to_ipa as ipa\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding, RepeatVector\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import load_model\n",
    "from keras import optimizers\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras.models import  Sequential, Model\n",
    "from tensorflow.keras.layers import Layer, Concatenate, Input, Masking, LSTM, Embedding, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data (data file already created in data_import.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('words.txt','r')\n",
    "lines = file.readlines()\n",
    "file.close()\n",
    "\n",
    "file = open('data.csv','w')\n",
    "\n",
    "file.write('word,pronunciation\\n')\n",
    "for word in lines:\n",
    "    if ipa.isin_cmu(word):\n",
    "        line = word.strip('\\n') + ',' + ipa.convert(word) + '\\n'\n",
    "        file.write(line)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of initial data: 40315\n",
      "Train size: 32253\n",
      "Val size: 4031\n",
      "Test size: 4031\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4031, 22)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv ('data.csv',delimiter=',')\n",
    "\n",
    "df.word = df.word.astype(str) \n",
    "df.pronunciation = df.pronunciation.astype(str) \n",
    "\n",
    "#df.applymap(lambda s: s.replace(s,' '.join(str(s))))\n",
    "df['word'] = df['word'].str.replace('',' ')\n",
    "df['pronunciation'] = df['pronunciation'].str.replace('',' ')\n",
    "\n",
    "MAX_NUM_WORDS = 40000\n",
    "\n",
    "# 10% for val, 10% for test, 70% for train\n",
    "val_size = int(df.shape[0] * 0.1)\n",
    "test_size = int(df.shape[0] * 0.1)\n",
    "\n",
    "# Shuffle the data\n",
    "df = df.sample(frac=1)\n",
    "# Split df to test/val/train\n",
    "test_df = df[:test_size]\n",
    "val_df = df[test_size:test_size+val_size]\n",
    "train_df = df[test_size+val_size:]\n",
    "\n",
    "\n",
    "train_words, train_pronounciations = list(train_df.word), list(train_df.pronunciation)\n",
    "val_words, val_pronounciations     = list(val_df.word), list(val_df.pronunciation)\n",
    "test_words, test_pronounciations   = list(test_df.word), list(test_df.pronunciation)\n",
    "\n",
    "\n",
    "# Check that idces do not overlap\n",
    "assert set(train_df.index).intersection(set(val_df.index)) == set({})\n",
    "assert set(test_df.index).intersection(set(train_df.index)) == set({})\n",
    "assert set(val_df.index).intersection(set(test_df.index)) == set({})\n",
    "# Check that all idces are present\n",
    "assert df.shape[0] == len(train_pronounciations) + len(val_pronounciations) + len(test_pronounciations)\n",
    "\n",
    "# Sizes\n",
    "print(\n",
    "    f\"Size of initial data: {df.shape[0]}\\n\"\n",
    "    f\"Train size: {len(train_pronounciations)}\\n\"\n",
    "    f\"Val size: {len(val_pronounciations)}\\n\"\n",
    "    f\"Test size: {len(test_prounciations)}\\n\"\n",
    ")\n",
    "\n",
    "for i in range(len(train_pronounciations)):\n",
    "    train_pronounciations[i] = \"<START>\" + train_pronounciations[i] + \"<END>\"\n",
    "\n",
    "for i in range(len(val_pronounciations)):\n",
    "    val_pronounciations[i] = \"<START>\" + val_pronounciations[i] + \"<END>\"\n",
    "\n",
    "for i in range(len(test_pronounciations)):\n",
    "    test_pronounciations[i] = \"<START>\" + test_pronounciations[i] + \"<END>\"\n",
    "\n",
    "ipa_tokenizer = Tokenizer(num_words = MAX_NUM_WORDS, filters = '')\n",
    "ipa_tokenizer.fit_on_texts(train_pronounciations)\n",
    "ipa_int_seq = ipa_tokenizer.texts_to_sequences(train_pronounciations)\n",
    "\n",
    "ipa_word_to_indx = ipa_tokenizer.word_index\n",
    "\n",
    "max_ipa_len = max(len(sen) for sen in ipa_int_seq)\n",
    "\n",
    "padded_tokenized_ipa = tf.keras.preprocessing.sequence.pad_sequences(ipa_int_seq, maxlen = max_ipa_len, padding = 'post', value = 0)\n",
    "\n",
    "padded_tokenized_ipa.shape\n",
    "\n",
    "\n",
    "\n",
    "val_tokenizer = Tokenizer(num_words = MAX_NUM_WORDS, filters = '')\n",
    "val_tokenizer.fit_on_texts(val_pronounciations)\n",
    "val_int_seq = val_tokenizer.texts_to_sequences(val_pronounciations)\n",
    "\n",
    "val_word_to_indx = ipa_tokenizer.word_index\n",
    "\n",
    "max_val_len = max(len(sen) for sen in val_int_seq)\n",
    "\n",
    "padded_tokenized_val = tf.keras.preprocessing.sequence.pad_sequences(val_int_seq, maxlen = max_val_len, padding = 'post', value = 0)\n",
    "\n",
    "padded_tokenized_val.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(TensorSpec(shape=(None, None, 128), dtype=tf.float32, name=None), TensorSpec(shape=(None, 24), dtype=tf.int32, name=None))\n",
      "(TensorSpec(shape=(None, None, 128), dtype=tf.float32, name=None), TensorSpec(shape=(None, 22), dtype=tf.int32, name=None))\n",
      "(16, 13, 128)\n",
      "tf.Tensor(\n",
      "[[ 1  3 36 18 30 14  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 1  3 11 19 10  4 23  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 1  3 36 32 10  5 31  4  8 15  2  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 1  3 10 25  5 23 12  7 25  5 17  2  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 1 18  3 23 14  7  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 1  3  6  4 16 35  4  6 13  2  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 1  9  7 19 31  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 1  6 18  9  7  9  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 1 11  4  8  3 23  5 31 37  4  6  2  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 1  3  6 25  5 12  8 28 22  2  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 1 11  6 25  5  4  6  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 1  3  9 32 17 28 22  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 1  3 20 22 31 14 12 17 19  8  2  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 1  3  9  7  6 19  7  4  2  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 1  3 11  5 35  4  8 12 34 18  6  2  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 1 36 21  5  7  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]], shape=(16, 22), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_data = tf.data.Dataset.from_tensor_slices((train_words, padded_tokenized_ipa))\n",
    "valid_data = tf.data.Dataset.from_tensor_slices((val_words, padded_tokenized_val))\n",
    "\n",
    "def str_split(e, g):\n",
    "    e = tf.strings.split(e)\n",
    "    return e, g\n",
    " \n",
    "train_data = train_data.map(str_split)\n",
    "valid_data = valid_data.map(str_split)\n",
    "       \n",
    "embedding_layer = hub.load(\"https://tfhub.dev/google/tf2-preview/nnlm-en-dim128-with-normalization/1\")    \n",
    "    \n",
    "def embed_english(x, y):\n",
    "    return embedding_layer(x), y\n",
    " \n",
    "train_data = train_data.map(embed_english)\n",
    "valid_data = valid_data.map(embed_english)\n",
    "\n",
    "\n",
    "def remove_long_sentence(e, g):\n",
    "    return tf.shape(e)[0] <= 13\n",
    " \n",
    "train_data = train_data.filter(remove_long_sentence)\n",
    "valid_data = valid_data.filter(remove_long_sentence)\n",
    "\n",
    "\n",
    "def pad_english(e, g):\n",
    "    return tf.pad(e, paddings = [[13-tf.shape(e)[0],0], [0,0]], mode='CONSTANT', constant_values=0), g\n",
    " \n",
    "train_data = train_data.map(pad_english)\n",
    "valid_data = valid_data.map(pad_english)\n",
    "\n",
    "train_data = train_data.batch(16)\n",
    "valid_data = valid_data.batch(16)\n",
    "\n",
    "print(train_data.element_spec)\n",
    "print(valid_data.element_spec)\n",
    "\n",
    "for e, g in train_data.take(1):\n",
    "    print(e.shape)\n",
    " \n",
    "for e, g in valid_data.take(1):\n",
    "    print(g)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLayer(Layer):\n",
    " \n",
    "    def __init__(self, **kwargs):\n",
    "        super(CustomLayer, self).__init__(**kwargs)\n",
    "        self.embed = tf.Variable(initial_value=tf.zeros(shape=(1,128)), trainable=True, dtype='float32')\n",
    "         \n",
    "    def call(self, inputs):\n",
    "        x = tf.tile(self.embed, [tf.shape(inputs)[0], 1])\n",
    "        x = tf.expand_dims(x, axis=1)\n",
    "        return tf.concat([inputs, x], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 13, 128)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([16, 14, 128])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_layer = CustomLayer()\n",
    "e, g = next(iter(train_data.take(1)))\n",
    "print(e.shape)\n",
    "o = custom_layer(e)\n",
    "o.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(batch_shape = (None, 13, 128), name='input')\n",
    "x = CustomLayer(name='custom_layer')(inputs)\n",
    "x = Masking(mask_value=0, name='masking_layer')(x)\n",
    "x, h, c = LSTM(units=512, return_state=True, name='lstm')(x)\n",
    "encoder_model = Model(inputs = inputs, outputs = [h, c], name='encoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "#tokenizer = Tokenizer(train_words)\n",
    "#pronounciation_tokenizer = Tokenizer(train_pronounciations)\n",
    "#print(word_tokenizer.index_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 22, 39) (16, 512) (16, 512)\n"
     ]
    }
   ],
   "source": [
    "class Decoder(Model):\n",
    "     \n",
    "    def __init__(self, **kwargs):\n",
    "        super(Decoder, self).__init__(**kwargs)\n",
    "        self.embed = Embedding(input_dim=len(ipa_tokenizer.index_word)+1, output_dim=128, mask_zero=True, name='embedding_layer')\n",
    "        self.lstm = LSTM(units = 512, return_state = True, return_sequences = True, name='lstm_layer')\n",
    "        self.dense = Dense(len(ipa_tokenizer.index_word)+1, name='dense_layer')\n",
    "         \n",
    "    def call(self, inputs, hidden_state = None, cell_state = None):\n",
    "        x = self.embed(inputs)\n",
    "        x, hidden_state, cell_state = self.lstm(x, initial_state = [hidden_state, cell_state]) \\\n",
    "                                                     if hidden_state is not None and cell_state is not None else self.lstm(x)\n",
    "        x = self.dense(x)\n",
    "        return x, hidden_state, cell_state\n",
    " \n",
    "decoder_model = Decoder(name='decoder')\n",
    "e, g_in = next(iter(train_data.take(1)))\n",
    "h, c = encoder_model(e)\n",
    "g_out, h, c = decoder_model(g_in, h, c)\n",
    " \n",
    "print(g_out.shape, h.shape, c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node 'decoder/embedding_layer/embedding_lookup' defined at (most recent call last):\n    File \"f:\\college stuff\\python\\lib\\runpy.py\", line 194, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"f:\\college stuff\\python\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"f:\\college stuff\\python\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"f:\\college stuff\\python\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n      app.start()\n    File \"f:\\college stuff\\python\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"f:\\college stuff\\python\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"f:\\college stuff\\python\\lib\\asyncio\\base_events.py\", line 570, in run_forever\n      self._run_once()\n    File \"f:\\college stuff\\python\\lib\\asyncio\\base_events.py\", line 1859, in _run_once\n      handle._run()\n    File \"f:\\college stuff\\python\\lib\\asyncio\\events.py\", line 81, in _run\n      self._context.run(self._callback, *self._args)\n    File \"f:\\college stuff\\python\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 504, in dispatch_queue\n      await self.process_one()\n    File \"f:\\college stuff\\python\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 493, in process_one\n      await dispatch(*args)\n    File \"f:\\college stuff\\python\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 400, in dispatch_shell\n      await result\n    File \"f:\\college stuff\\python\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 724, in execute_request\n      reply_content = await reply_content\n    File \"f:\\college stuff\\python\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 390, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"f:\\college stuff\\python\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"f:\\college stuff\\python\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2863, in run_cell\n      result = self._run_cell(\n    File \"f:\\college stuff\\python\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2909, in _run_cell\n      return runner(coro)\n    File \"f:\\college stuff\\python\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"f:\\college stuff\\python\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3106, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"f:\\college stuff\\python\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3309, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"f:\\college stuff\\python\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3369, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\jojoy\\AppData\\Local\\Temp\\ipykernel_35420\\1793995889.py\", line 63, in <cell line: 63>\n      train_loss_results, valid_loss_results = train_encoder_decoder(encoder_model, decoder_model, 20, train_data, valid_data, 20,\n    File \"C:\\Users\\jojoy\\AppData\\Local\\Temp\\ipykernel_35420\\1793995889.py\", line 49, in train_encoder_decoder\n      train_loss, grads = grad_fn(encoder_model, decoder_model, e, g_in, g_out, loss)\n    File \"C:\\Users\\jojoy\\AppData\\Local\\Temp\\ipykernel_35420\\712443347.py\", line 34, in forward_backward\n      d_g_out, _, _ = decoder_model(g_in, h, c)\n    File \"f:\\college stuff\\python\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"f:\\college stuff\\python\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1096, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"f:\\college stuff\\python\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\jojoy\\AppData\\Local\\Temp\\ipykernel_35420\\1055797883.py\", line 10, in call\n      x = self.embed(inputs)\n    File \"f:\\college stuff\\python\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"f:\\college stuff\\python\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1096, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"f:\\college stuff\\python\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"f:\\college stuff\\python\\lib\\site-packages\\keras\\layers\\embeddings.py\", line 197, in call\n      out = tf.nn.embedding_lookup(self.embeddings, inputs)\nNode: 'decoder/embedding_layer/embedding_lookup'\nindices[13,2] = 40 is not in [0, 39)\n\t [[{{node decoder/embedding_layer/embedding_lookup}}]] [Op:__inference_forward_backward_110982]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Input \u001b[1;32mIn [124]\u001b[0m, in \u001b[0;36m<cell line: 63>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     61\u001b[0m optimizer_obj \u001b[38;5;241m=\u001b[39m Adam(learning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-3\u001b[39m)\n\u001b[0;32m     62\u001b[0m loss_obj \u001b[38;5;241m=\u001b[39m SparseCategoricalCrossentropy(from_logits\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 63\u001b[0m train_loss_results, valid_loss_results \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_encoder_decoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m                                                          \u001b[49m\u001b[43moptimizer_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_backward\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [124]\u001b[0m, in \u001b[0;36mtrain_encoder_decoder\u001b[1;34m(encoder_model, decoder_model, num_epochs, train_data, valid_data, valid_steps, optimizer, loss, grad_fn)\u001b[0m\n\u001b[0;32m     47\u001b[0m g_in, g_out \u001b[38;5;241m=\u001b[39m get_ipa_decoder_data(g)\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m#print(g_out)\u001b[39;00m\n\u001b[1;32m---> 49\u001b[0m train_loss, grads \u001b[38;5;241m=\u001b[39m \u001b[43mgrad_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mg_in\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mg_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mapply_gradients(\u001b[38;5;28mzip\u001b[39m(grads, encoder_model\u001b[38;5;241m.\u001b[39mtrainable_variables \u001b[38;5;241m+\u001b[39m decoder_model\u001b[38;5;241m.\u001b[39mtrainable_variables))\n\u001b[0;32m     51\u001b[0m train_epoch_loss_avg\u001b[38;5;241m.\u001b[39mupdate_state(train_loss)    \n",
      "File \u001b[1;32mf:\\college stuff\\python\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mf:\\college stuff\\python\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node 'decoder/embedding_layer/embedding_lookup' defined at (most recent call last):\n    File \"f:\\college stuff\\python\\lib\\runpy.py\", line 194, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"f:\\college stuff\\python\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"f:\\college stuff\\python\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"f:\\college stuff\\python\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n      app.start()\n    File \"f:\\college stuff\\python\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"f:\\college stuff\\python\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"f:\\college stuff\\python\\lib\\asyncio\\base_events.py\", line 570, in run_forever\n      self._run_once()\n    File \"f:\\college stuff\\python\\lib\\asyncio\\base_events.py\", line 1859, in _run_once\n      handle._run()\n    File \"f:\\college stuff\\python\\lib\\asyncio\\events.py\", line 81, in _run\n      self._context.run(self._callback, *self._args)\n    File \"f:\\college stuff\\python\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 504, in dispatch_queue\n      await self.process_one()\n    File \"f:\\college stuff\\python\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 493, in process_one\n      await dispatch(*args)\n    File \"f:\\college stuff\\python\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 400, in dispatch_shell\n      await result\n    File \"f:\\college stuff\\python\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 724, in execute_request\n      reply_content = await reply_content\n    File \"f:\\college stuff\\python\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 390, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"f:\\college stuff\\python\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"f:\\college stuff\\python\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2863, in run_cell\n      result = self._run_cell(\n    File \"f:\\college stuff\\python\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2909, in _run_cell\n      return runner(coro)\n    File \"f:\\college stuff\\python\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"f:\\college stuff\\python\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3106, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"f:\\college stuff\\python\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3309, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"f:\\college stuff\\python\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3369, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\jojoy\\AppData\\Local\\Temp\\ipykernel_35420\\1793995889.py\", line 63, in <cell line: 63>\n      train_loss_results, valid_loss_results = train_encoder_decoder(encoder_model, decoder_model, 20, train_data, valid_data, 20,\n    File \"C:\\Users\\jojoy\\AppData\\Local\\Temp\\ipykernel_35420\\1793995889.py\", line 49, in train_encoder_decoder\n      train_loss, grads = grad_fn(encoder_model, decoder_model, e, g_in, g_out, loss)\n    File \"C:\\Users\\jojoy\\AppData\\Local\\Temp\\ipykernel_35420\\712443347.py\", line 34, in forward_backward\n      d_g_out, _, _ = decoder_model(g_in, h, c)\n    File \"f:\\college stuff\\python\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"f:\\college stuff\\python\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1096, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"f:\\college stuff\\python\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\jojoy\\AppData\\Local\\Temp\\ipykernel_35420\\1055797883.py\", line 10, in call\n      x = self.embed(inputs)\n    File \"f:\\college stuff\\python\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"f:\\college stuff\\python\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1096, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"f:\\college stuff\\python\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"f:\\college stuff\\python\\lib\\site-packages\\keras\\layers\\embeddings.py\", line 197, in call\n      out = tf.nn.embedding_lookup(self.embeddings, inputs)\nNode: 'decoder/embedding_layer/embedding_lookup'\nindices[13,2] = 40 is not in [0, 39)\n\t [[{{node decoder/embedding_layer/embedding_lookup}}]] [Op:__inference_forward_backward_110982]"
     ]
    }
   ],
   "source": [
    "def get_ipa_decoder_data(g):\n",
    "    g1 = g.numpy()\n",
    "    #print(g1)\n",
    "    #new_g1 =  np.delete(g1, np.where(g1 == 2), axis = 0)\n",
    "    '''\n",
    "    for list in g1:\n",
    "        for int in range(len(list)):\n",
    "            if list[int] == 2:\n",
    "                np.delete(list, [int])\n",
    "    '''\n",
    "    \n",
    "    x, y = g1.shape\n",
    "    #after delete\n",
    "    new_g1 = g1[g1 != 2]\n",
    "    new_g1 = np.reshape(new_g1, (x, y-1))\n",
    "\n",
    "    #print(new_g1)\n",
    "    \n",
    "    g2 = g.numpy()\n",
    "    new_g2 =  np.delete(g2,0, axis = 1)\n",
    "    new_g1 = np.pad(new_g1, ((0,0),(1,0)), 'constant')\n",
    "    g_in = tf.convert_to_tensor(new_g1, dtype=tf.int32)\n",
    "    \n",
    "    #print(g_in)\n",
    "    new_g2 = np.pad(new_g2, ((0,0),(0,1)), 'constant')\n",
    "    g_out = tf.convert_to_tensor(new_g2, dtype=tf.int32)\n",
    "    #print(g_out)\n",
    "    return g_in, g_out\n",
    "\n",
    "@tf.function\n",
    "def forward_backward(encoder_model, decoder_model, e, g_in, g_out, loss):\n",
    "    with tf.GradientTape() as tape:\n",
    "        h, c = encoder_model(e)\n",
    "        d_g_out, _, _ = decoder_model(g_in, h, c)\n",
    "        cur_loss = loss(g_out, d_g_out)\n",
    "        grads = tape.gradient(cur_loss, encoder_model.trainable_variables + decoder_model.trainable_variables)\n",
    "    return cur_loss, grads\n",
    " \n",
    "def train_encoder_decoder(encoder_model, decoder_model, num_epochs, train_data, valid_data, valid_steps, \n",
    "                          optimizer, loss, grad_fn):\n",
    "    train_losses = []\n",
    "    val_loasses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        train_epoch_loss_avg = tf.keras.metrics.Mean()\n",
    "        val_epoch_loss_avg = tf.keras.metrics.Mean()\n",
    "        for e, g in train_data:\n",
    "            g_in, g_out = get_ipa_decoder_data(g)\n",
    "            #print(g_out)\n",
    "            train_loss, grads = grad_fn(encoder_model, decoder_model, e, g_in, g_out, loss)\n",
    "            optimizer.apply_gradients(zip(grads, encoder_model.trainable_variables + decoder_model.trainable_variables))\n",
    "            train_epoch_loss_avg.update_state(train_loss)    \n",
    "        for e_v, g_v in valid_data.take(valid_steps):\n",
    "            g_v_in, g_v_out = get_german_decoder_data(g_v)\n",
    "            val_loss, _ = grad_fn(encoder_model, decoder_model, e_v, g_v_in, g_v_out, loss)\n",
    "            val_epoch_loss_avg.update_state(val_loss)        \n",
    "        print(f'epoch: {epoch}, train loss: {train_epoch_loss_avg.result()}, validation loss: {val_epoch_loss_avg.result()}')    \n",
    "        train_losses.append(train_epoch_loss_avg.result())\n",
    "        val_loasses.append(val_epoch_loss_avg.result())\n",
    "    return train_losses, val_loasses\n",
    " \n",
    "optimizer_obj = Adam(learning_rate = 1e-3)\n",
    "loss_obj = SparseCategoricalCrossentropy(from_logits=True)\n",
    "train_loss_results, valid_loss_results = train_encoder_decoder(encoder_model, decoder_model, 20, train_data, valid_data, 20,\n",
    "                                                          optimizer_obj, loss_obj, forward_backward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize\n",
    "#word_tokenizer = Tokenizer(train_words)\n",
    "#pronounciation_tokenizer = Tokenizer(train_pronounciations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = Sequential()\n",
    "#model.add(Embedding(len(train_words), 512, input_length=20, mask_zero=True))\n",
    "#model.add(LSTM(512))\n",
    "#model.add(RepeatVector(20))\n",
    "#model.add(LSTM(512, return_sequences=True))\n",
    "#model.add(Dense(len(train_pronounciations), activation='softmax'))\n",
    " \n",
    "#rms = keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "#model.compile(optimizer=rms, loss='sparse_categorical_crossentropy')\n",
    "\n",
    "#model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6a251302415591a00991e803008012c897957666279c59a7b2e49cbcc4b50ee2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
